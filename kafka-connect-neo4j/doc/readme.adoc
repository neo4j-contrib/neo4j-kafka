= Build it locally

Build the project by running the following command:

    mvn clean install

Inside the directory `<neo4j-streams>/kafka-connect-neo4j/target/component/packages` you'll find a file named `neo4j-kafka-connect-neo4j-<VERSION>.zip`

== Sink

=== Configuring the stack

Create a directory `plugins` at the same level of the compose file and unzip the file `neo4j-kafka-connect-neo4j-<VERSION>.zip` inside it, then start the compose file

    docker-compose up -d

Create the Sink instance:

We'll define the Sink configuration as follows:

[source,json]
----
include::contrib.sink.avro.neo4j.json[]
----

In particular this line:

----
"neo4j.topic.cypher.my-topic": "MERGE (p:Person{name: event.name, surname: event.surname}) MERGE (f:Family{name: event.surname}) MERGE (p)-[:BELONGS_TO]->(f)"
----

defines that all the data that comes from the topic `neo4j` will be unpacked by the Sink into Neo4j with the following Cypher query:

[source,cypher]
----
MERGE (p:Person{name: event.name, surname: event.surname})
MERGE (f:Family{name: event.surname})
MERGE (p)-[:BELONGS_TO]->(f)
----


Under the hood the Sink inject the event object in this way

[source,cypher]
----
UNWIND {batch} AS event
MERGE (p:Person{name: event.name, surname: event.surname})
MERGE (f:Family{name: event.surname})
MERGE (p)-[:BELONGS_TO]->(f)
----

Where `{batch}` is a list of event objects.

You can change the query or remove the property and add your own, but you must follow the following convention:

[source,javascript]
----
"neo4j.topic.cypher.<YOUR_TOPIC>": "<YOUR_CYPHER_QUERY>"
----

Let's load the configuration into the Confluent Platform with this REST call:

[source,shell]
----
curl -X POST http://localhost:8083/connectors \
  -H 'Content-Type:application/json' \
  -H 'Accept:application/json' \
  -d @contrib.sink.avro.neo4j.json
----

The file `contrib.sink.string-json.neo4j.json` contains a configuration that manage a simple JSON producer example

Please check that everything is fine by going into:

http://localhost:9021/management/connect

and click to the **Sink** tab. You must find a table just like this:

[cols="4*",options="header"]
|===
|Status
|Active Tasks
|Name
|Topics

|Running
|1
|Neo4jSinkConnector
|my-topic
|===

=== Use the data generator

You can download and use the https://github.com/conker84/neo4j-streams-sink-tester/releases/download/1/neo4j-streams-sink-tester-1.0.jar[neo4j-streams-sink-tester-1.0.jar] in order to generate a sample dataset.

This package sends records to the Neo4j Kafka Sink by using the following in two data formats:

JSON example:

[source,json]
----
{"name": "Name", "surname": "Surname"}
----

AVRO, with the schema:

[source,json]
----
{
 "type":"record",
 "name":"User",
 "fields":[{"name":"name","type":"string"}, {"name":"surname","type":"string"}]
}
----

Please type:

----
java -jar neo4j-streams-sink-tester-1.0.jar -h
----

to print the option list with default values.

In order to choose the data format please use the `-f` flag: `-f AVRO` or `-f JSON` (the default value).
So:

----
java -jar neo4j-streams-sink-tester-1.0.jar -f AVRO
----

Will send data in AVRO format.

For a complete overview of the **Neo4j Steams Sink Tester** please refer to https://github.com/conker84/neo4j-streams-sink-tester[this repo]

== Source

=== Configuring the stack

Create a directory `plugins` at the same level of the compose file and unzip the file `neo4j-kafka-connect-neo4j-<VERSION>.zip` inside it, then start the compose file

    docker-compose up -d

=== Create the Source instance:

In this chapter we'll discuss about how the Source instance works

You can create a new Source instance with this REST call:

[source,shell]
----
curl -X POST http://localhost:8083/connectors \
  -H 'Content-Type:application/json' \
  -H 'Accept:application/json' \
  -d @contrib.source.avro.neo4j.json
----

Let's look at the `contrib.source.avro.neo4j.json` file:

[source,json]
----
include::contrib.source.avro.neo4j.json[]
----

This will create a Kafka Connect Source instance that will send `AVRO` message over the topic named `my-topic`. Every message in the
topic will have the following structure:

[source,json]
----
{"name": <name>, "timestamp": <timestamp>}
----

**Nb.** Please check the <<kafka-connect-configuration-summary>> for a detailed guide about the supported configuration
parameters

=== How the Source module pushes the data to the defined Kafka topic

We use the query provided in the `neo4j.source.query` field by polling the database every value is into the
`neo4j.streaming.poll.interval.msecs` field.

So given the JSON configuration we have that we'll perform:

[source,cypher]
----
MATCH (ts:TestSource) WHERE ts.timestamp > $lastCheck RETURN ts.name AS name, ts.timestamp AS timestamp
----

every 5000 milliseconds by publishing events like:

[source,json]
----
{"name":{"string":"John Doe"},"timestamp":{"long":1624551349362}}
----

In this case we use `neo4j.enforce.schema=true` and this means that we will attach a schema for each record, in case
you want to stream pure simple JSON strings just use the relative serializer with `neo4j.enforce.schema=false` with the
following output:

[source,json]
----
{"name": "John Doe", "timestamp": 1624549598834}
----