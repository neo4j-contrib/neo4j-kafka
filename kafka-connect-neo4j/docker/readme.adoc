=== Configuration parameters

You can set the following configuration values via Confluent Connect UI, or via REST endpoint

[cols="3*",options="header"]
|===
|Field|Type|Description

|neo4j.server.uri|String|The Bolt URI
|neo4j.authentication.type|enum[NONE, BASIC, KERBEROS]| The authentication type (default BASIC)
|neo4j.batch.size|Int|The max number of events processed by the Cypher query (default 1000)
|neo4j.batch.timeout.msec|Long|The timeout after that the batched data will be returned if the size is not achieved (default 30000)
|neo4j.query.timeout.msec|Long|The execution timeout for the cypher query (default 50000)
|neo4j.authentication.basic.username|String| The authentication username
|neo4j.authentication.basic.password|String| The authentication password
|neo4j.authentication.basic.realm|String| The authentication realm
|neo4j.authentication.kerberos.ticket|String| The Kerberos ticket
|neo4j.encryption.enabled|Boolean| If the encryption is enabled (default false)
|neo4j.encryption.trust.strategy|enum[TRUST_ALL_CERTIFICATES,TRUST_CUSTOM_CA_SIGNED_CERTIFICATES,TRUST_SYSTEM_CA_SIGNED_CERTIFICATES]| The Neo4j trust strategy (default TRUST_ALL_CERTIFICATES)
|neo4j.encryption.ca.certificate.path|String| The path of the certificate
|neo4j.connection.max.lifetime.msecs|Long| The max Neo4j connection lifetime (default 1 hour)
|neo4j.connection.acquisition.timeout.msecs|Long| The max Neo4j acquisition timeout (default 1 hour)
|neo4j.connection.liveness.check.timeout.msecs|Long| The max Neo4j liveness check timeout (default 1 hour)
|neo4j.connection.max.pool.size|Int| The max pool size (default 100)
|neo4j.load.balance.strategy|enum[ROUND_ROBIN,LEAST_CONNECTED]| The Neo4j load balance strategy (default LEAST_CONNECTED)
|===

=== Build it locally

Build the project by running the following command:

    mvn clean install

Inside the directory `<neo4j-streams>/kafka-connect-neo4j/target/component/packages` you'll find a file named `neo4j-kafka-connect-neo4j-<VERSION>.zip`

=== Configuring the stack

Create a directory `plugins` at the same level of the compose file and unzip the file `neo4j-kafka-connect-neo4j-<VERSION>.zip` inside it, then start the compose file

    docker-compose up -d

You can access your Neo4j instance under: http://localhost:7474, log in with `neo4j` as username and `connect` as password (see the docker-compose file to change it).

The insertion is sped up, if you create these two indexes:

[source,cypher]
----
CREATE INDEX ON :Person(surname);
CREATE CONSTRAINT ON (f:Family) ASSERT f.name IS UNIQUE;
----

Create the Sink instance:

We'll define the Sink configuration as follows:

[source,json]
----
include::contrib.sink.avro.neo4j.json[]
----

In particular this line:

----
"neo4j.topic.cypher.my-topic": "MERGE (p:Person{name: event.name, surname: event.surname}) MERGE (f:Family{name: event.surname}) MERGE (p)-[:BELONGS_TO]->(f)"
----

defines that all the data that comes from the topic `my-topic` will be unpacked by the Sink into Neo4j with the following Cypher query:

[source,cypher]
----
MERGE (p:Person{name: event.name, surname: event.surname})
MERGE (f:Family{name: event.surname})
MERGE (p)-[:BELONGS_TO]->(f)
----


Under the hood the Sink inject the event object in this way

[source,cypher]
----
UNWIND {batch} AS event
MERGE (p:Person{name: event.name, surname: event.surname})
MERGE (f:Family{name: event.surname})
MERGE (p)-[:BELONGS_TO]->(f)
----

Where `{batch}` is a list of event objects.

You can change the query or remove the property and add your own, but you must follow the following convention:

[source,javascript]
----
"neo4j.topic.cypher.<YOUR_TOPIC>": "<YOUR_CYPHER_QUERY>"
----

Let's load the configuration into the Confluent Platform with this REST call:

[source,shell]
----
curl -X POST http://localhost:8083/connectors \
  -H 'Content-Type:application/json' \
  -H 'Accept:application/json' \
  -d @contrib.sink.avro.neo4j.json
----

The file `contrib.sink.string-json.neo4j.json` contains a configuration that manage a simple JSON producer example

Please check that everything is fine by going into:

http://localhost:9021/management/connect

(or on 0.0.0.0 instead of localhost depending on your Docker environment)

and click to the **Send data out** (or **Sink** depending on your Confluent Platform Version) tab. You must find a table just like this:

[cols="4*",options="header"]
|===
|Status
|Active Tasks
|Name
|Topics

|Running
|1
|Neo4jSinkConnector
|my-topic
|===

=== Use the data generator

You can download and use the https://github.com/conker84/neo4j-streams-sink-tester/releases/download/1/neo4j-streams-sink-tester-1.0.jar[neo4j-streams-sink-tester-1.0.jar] in order to generate a sample dataset.

This package sends records to the Neo4j Kafka Sink by using the following in two data formats:

JSON example:

[source,json]
----
{"name": "Name", "surname": "Surname"}
----

AVRO, with the schema:

[source,json]
----
{
 "type":"record",
 "name":"User",
 "fields":[{"name":"name","type":"string"}, {"name":"surname","type":"string"}]
}
----

_Note_: Before start using the data generator please create the following indexes on Neo4j (in order to speed-up the import process):

[source,cypher]
----
CREATE INDEX ON :Person(name)
----

[source,cypher]
----
CREATE INDEX ON :Family(surname)
----

Please type:

----
java -jar neo4j-streams-sink-tester-1.0.jar -h
----

to print the option list with default values.

In order to choose the data format please use the `-f` flag: `-f AVRO` or `-f JSON` (the default value).
So:

----
java -jar neo4j-streams-sink-tester-1.0.jar -f AVRO
----

Will send data in AVRO format.

For a complete overview of the **Neo4j Steams Sink Tester** please refer to https://github.com/conker84/neo4j-streams-sink-tester[this repo]

