= Neo4j Kafka Integrations



== Neo4j-Kafka

Stream transaction event handler events to a Kafka topic

Ideas based on a now removed repository from workday.

Currently for kafka


=== Installation

Build locally
// todo release

----
mvn clean install
----

2. Copy `target/neo4j-kafka-*.jar` into `$NEO4J_HOME/plugins`
3. Restart Neo4j

=== Configuration

The Kafka module contains the following configurations key (with defaults):

* *kafka.bootstrap.servers*: _Default value: localhost:9092_
* *kafka.acks*: _Default value: 1_
* *kafka.retries*: _Default value: 2_
* *kafka.partition.size*: _Default value: 1_
* *kafka.linger.ms*: _Default value: 1_
* *kafka.batch.size*: _Default value: 16384_
* *kafka.buffer.memory*: _Default value: 33554432_
* *kafka.topic*: _Default value: neo4j_
* *kafka.replication*: _Default value: 1_
* *kafka.group.id*: _Default value: neo4j_

For the description of every single proprerty you can count on the offcial https://kafka.apache.org/documentation/#brokerconfigs[Apache Kafka Documentation]


=== Testing

Following https://kafka.apache.org/quickstart[these instructions].

1. Download & Unzip Kafka

2. Run Zookeeper, server and a test consumer.

----
bin/zookeeper-server-start.sh config/zookeeper.properties

bin/kafka-server-start.sh config/server.properties

bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic neo4j [--from-beginning]
----

=== Next Steps

* Send data asynchronously
* Use Kafka transactions for batching updates


== Neo4j-Kafka-Consumer

1. Read Kafka records from given topic(s)
2. Treat records as input parameters
3. Uses configured Cypher statements to merge them into the graph.

=== Next Steps

1. Implement as Kernel Extension
2. Implement as Bolt Client

== Neo4j-Kafka-Connect

Implementation for a Kafka Sink / Source

based on work in https://github.com/pegerto/kafka-connect-neo4j

=== Next Steps

1. Implement for Bolt connection
2. Figure out if kernel extension makes sense
